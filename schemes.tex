%!TEX root = raspi_journal.tex
\label{sec:schemes}

In this section, we present a description of the considered coding schemes
measured with our the Raspberry Pi testbed. We introduce a definition
for the primitive coding operations, e.g. encoding, decoding and recoding
(where applies) for each coding scheme. Later, we address particular schemes
which are obtained by modifying the basic coding operations that provide
better processing speeds, particularly relevant for the Raspberry Pi.

\subsection{Random Linear Network Coding}
\label{ssec:RLNC}

\ac{RLNC} is an example of intra-session \ac{NC}. Here, data symbols from a
single flow are combined with each other. In this type of network coding,
all the original data packets $P_j, j \in [1,g]$, each of $B$ bits, are used
to create coded packets. In the following subsections, we describe the
basic functionalities of \ac{RLNC}.

\subsubsection{Encoding}
In \ac{RLNC}, any coded packet is a linear combination from all
the original packets. For the coding scheme, packets are seen as
algebraic entities formed as a sequence of elements from $GF(q)$,
which is a \ac{GF} of size $q$. Later, each original packet is
multiplied by a coding coefficient from $GF(q)$. The coding coefficients
are picked uniformly at random from the \ac{GF} by the encoder. To
perform the multiplication of a packet by a coding coefficient, the
coefficient is multiplied for each of the elements in the
concatenation that composes an original packet, preserving the
concatenation. Later, all resulting packets are added within the
\ac{GF} arithmetics together to generate a coded packet. Thus, a
coded packet can be written as:
%
\begin{align} \label{eq:coded_packet}
C_i  &= \bigoplus_{j=1}^{g} v_{ij} \otimes P_j ,\ \forall i \in [1,\ldots)
\end{align}

In \eqref{eq:coded_packet}, $C_i$ is the generic coded packet. In principle,
the encoder may produce any number of coded packets, but a finite
number is produced in practice given that a decoder needs only
$g$ \ac{l.i.} coded packets to decode the batch. Also, in
\eqref{eq:coded_packet}, $v_{ij}$ is the coding coefficient used in the
$i$-th coded packet and assigned to multiply the $j$-th original packet.

For indicating a receiver which packets were utilized
to create a coded one, a simple yet versatile choice is to append
its coding coefficients as a header of the coded packet. Hence, an
amount of overhead is included in every coded packet given that we
need to provide some signalling needed for decoding. The coding
coefficients overhead amount for packet $i$, $|v_i|$, can be quantified
as:
%
\begin{align} \label{eq:coded_packet_coef}
|v_i| &= \sum_{j=1}^{g} |v_{ij}| = g \times \lceil \log_{2} (q)
\rceil ~ [\mathrm{bits}]
\end{align}

\subsubsection{Decoding}
\label{sssec:decoding}
To be able to decode a batch of $g$ packets, a \ac{l.i.} set of $g$
coded packets, $C_i,\ i \in [1,g]$ is required at a decoder.
Once this set has been collected for a decoder,
the original packets can be found by computing the solution of a system
of linear equations using \ac{GF} aritmethics. Thus, we define
$\textbf{C} = \left[C_1 \ldots C_g \right]^T$,
$\textbf{P} = \left[P_1 \ldots P_g \right]^T$ and the coding matrix
$\textbf{V}$ that collects the coding coefficients for each of the $g$
coded packets, as follows:
%
\begin{align} \label{eq:coding_matrix}
\textbf{V} =
\left[
\begin{array}{c}
        v_1    \\ \hline
        \vdots \\ \hline
        v_g    \\
\end{array}
\right]
=
\left[
\begin{array}{ccc}
        v_{11} & \ldots & v_{1g} \\
        \vdots  & \ddots & \vdots  \\
        v_{g1} & \ldots & v_{gg} \\
\end{array}
\right]
\end{align}
%
Algebraically, decoding simplifies to finding the inverse of
$\textbf{V}$ in the linear system
$\textbf{C} = \textbf{V} \textbf{P}$, which can achieved using
efficient Gaussian elimination techniques \cite{fragouli2006network}.
On real applications, decoding is performed on-the-fly, e.g. the pivots are
computed as packets are progressively received, in order to minimize the
computation delay for each step.

A decoder starts to calculate and substract contributions from each
of the pivot elements, e.g. leftmost elements in the main diagonal of
\eqref{eq:coding_matrix}, from top to bottom. The purpose is to obtain
the equivalence  $\textbf{V}$ in its reduced echelon form. The steps
for reducing the matrix by elementary row operations are carried each time
a \ac{l.i.} packet is received. Once in reduced echelon form, packets
can be retrieved by doing a substitution starting from the latest
coded packet. In this way, the amount of elementary operations at the end
of the decoding process is diminished.

\subsubsection{Recoding}
As an inherent property of \ac{RLNC}, an intermediate node in the network
is able to create new coded packets without needing to decode previously
received ones from an encoding source. Therefore, \ac{RLNC} is an end-to-end
coding scheme that permits to \textit{recode} former coded packets at any
point in the network without inquiring in local decoding processing. In
principle, a recoded packet should be indistinguishable from a coded one.
Thus, we define a recoded packet as $R_i$ and consider the coding
coefficients $w_{i1}, \ldots, w_{ig}$ as the used ones to
create $R_i$. Later, a recoded packet can be written as:
%
\begin{align}
\label{eq:recoded_packet}
R_i = \bigoplus_{j=1}^{g} w_{ij} \otimes C_j  ,\ \forall i \in [1,\ldots)
\end{align}

In \eqref{eq:recoded_packet}, $w_{ij}$ is the coding coefficient used
in the $i$-th recoded packet and assigned to multiply a previously coded
packet $C_j$. These coding coefficients are again uniformly and randomly chosen
from $GF(q)$. Still, each of the $w_{ij}$ are not the appended recoding
coefficients for signalling since the they will only retrieve previously
coded packets. The analytical expression for the appended coefficients
after recoding is obtained by calculating recoded packets as function of the
original packets. Therefore, we define the local coding matrix $\textbf{W}$
in the same way as it was made for $\textbf{V}$. Thus, the local coding matrix
can be written as:
%
\begin{align} \label{eq:local_coding_matrix}
\textbf{W} =
\left[
\begin{array}{c}
        w_1    \\ \hline
        \vdots \\ \hline
        w_g    \\
\end{array}
\right]
=
\left[
\begin{array}{ccc}
        w_{11} & \ldots & w_{1g} \\
        \vdots  & \ddots & \vdots  \\
        w_{g1} & \ldots & w_{gg} \\
\end{array}
\right]
\end{align}
%
With the definitions from \eqref{eq:coding_matrix} and
\eqref{eq:local_coding_matrix}, the recoded packets
$\textbf{R} = \left[R_1 \ldots R_g \right]^T$ are written as
$\textbf{R} = (\textbf{W} \textbf{V}) \textbf{P}$. Here, we recognize
the relationship between original and recoded packets. The resulting
coding matrix is simply the multiplication of matrices $\textbf{W}$ and
$\textbf{V}$. Denoting ${(\textbf{W} \textbf{V})}_{ij}$ as the element
in the $i$-th row and $j$-th column of $(\textbf{W} \textbf{V})$, this
term is the resulting coding coefficient used to create $R_i$ after
encoding the original packet $P_j$ recoding it locally in an intermediate
node. Finally, the appended coefficients for $R_i$ are
${(\textbf{W} \textbf{V})}_{ik}$ with $k \in [1,g]$. By doing some
algebra, each ${(\textbf{W} \textbf{V})}_{ij}$ term can be verified to be
computed as:
%
\begin{align}
\label{eq:appended_coded_coefficients}
{(\textbf{W} \textbf{V})}_{ij} = \sum_{k=1}^{g} w_{ik} v_{kj}\ ,\ \forall i,j \in [1,g] \times [1,g]
\end{align}
%
This procedure is carried by all the recoders in a given network, therefore
allowing any decoder to compute the original data after Gaussian elimination,
regardless of the amount of times where recoding was performed. Similarly
as with the encoding operation, any decoder that collects a set of
$g$ \ac{l.i.} recoded packets with their respective coefficients, will
be able to decode the data as mentioned before in Section \ref{sssec:decoding}.

\subsection{Sparse Random Linear Network Coding}

In \ac{SRLNC}, instead of considering all the packets to create a coded
packet as in \ac{RLNC}, for this coding scheme an encoder sets more
coding coefficients to zero when generating a coded packet with the purpose
of reducing the overall processing. Decoding is the same as in \ac{RLNC}, but
given that the coding matrices are now sparse, we will have less operations
performed in the decoding process. Recoding, although theoreticaly possible,
is omitted in practical systems since it is too complex to keep packets sparse
after recoding [\textbf{reference}]. In what follows, we
describe the coding scheme with a basic method to produce sparse coded
packets and later we indicate a second one, giving reference to their
implementation performance.

\subsubsection{Method 1: Fixing the Coding Density}
A way to control the amount of non-zero coding coefficients, is to set a
fixed ratio of non-zero coefficients in the encoding vector of size $g$. We
refer to this fixed ratio as the average coding density $d$. Thus, for any
coded packet $C_i$ with coding coefficients $v_{ij},\ j \in [1,g]$, its
coding density is defined as follows:
%
\begin{align}
\label{eq:coding_density}
d = \frac{\sum_{j=1}^{g} f(v_{ij})}{g},\ \
    f(v_{ij}) =
    \begin{cases}
        0 & \text{for } v_{ij} = 0 \\
        1 & \text{for } v_{ij} \neq 0
    \end{cases}
\end{align}
%
From the density definition in \eqref{eq:coding_density}, it can be
observed that $0 \leq d \leq 1$. As $g$ increased, we obtain more
granularity in the density. Notice that the special case $d = 0$ has no
practical purpose since it implies just zero padded data. So, a practical
range for the density excludes the zero case, $0 < d \leq 1$. Moreover,
it can be also observed that the case of $d = 1$ reduces to the \ac{RLNC}
coding scheme described in Section \ref{ssec:RLNC}.

To achieve a desired average coding density in the coefficients, for each
of them, we utilize a set of Bernoulli random variables all with parameter
$d$ as it success probability, i.e. $\mathbb{B}_j \sim Bernoulli(d)\ ,\ \forall
j \in [1,g]$. In this way, we can represent a coded packet in \ac{SRLNC} as:
%
\begin{align} \label{eq:sparse_coded_packet}
    C_i  &= \bigoplus_{j=1}^{g} \mathbb{B}_j v_{ij} \otimes P_j ,\ v_{ij}
    \neq 0\
    \forall i \in [1,\ldots) ,\ \
    d \in
    \begin{cases}
        {(0,0.5]} & \text{for } q = 2 \\
        {(0,1]} & \text{for } q > 2
    \end{cases}
\end{align}
%
In \eqref{eq:sparse_coded_packet}, we have the requirement for the coding
coefficient to not be zero, since we want to ensure a coding coefficient
is generate for any random trial where $\mathbb{B}_j = 1$. Therefore, in our
implementation of \ac{SRLNC} we exclude the zero element and then pick
uniformly distributed random elements from $GF(q)-\{ 0 \}$. Also, we have
specified a dependency on the field size for practical
density values. In case of employing $GF(2)$, the
maximum plausible density is restricted up to 0.5 since higher values incur
in more frequent \ac{l.d.} coded packets \textbf{Include reference here}.

Reducing $d$ enables the encoder to decrease the average number of packets
manipulated to make a coded one. This reduces the complexity of the encoder
since it needs to mix less packets. Moreover, it also simplifies the decoder
processing given that less nonzero coding coefficients are required to be
operated during the Gaussian elimination stage.

The drawback of this scheme is that coded packets from the encoder
become more \ac{l.d.} on each other as the density is reduced.
This leads to transmission overhead since another coded packet
is required to be sent for every reception of a redundant packet.
Also, particularly with this method, we may still generate a coded packet
which does not contain any data at all. For example, we might find
the case where $\left[\mathbb{B}_1, \ldots, \mathbb{B}_g \right] =
\mathbf{0}$. In that case, the encoder discards the coded packet
and tries to generate a new one. This situation occurs more frequently
as the density is reduced.

\subsubsection{Method 2: Sampling the Amount of Packets to Combine}
The method described from \eqref{eq:sparse_coded_packet} permits
to obtain a fast implementation in terms of execution time for
$d \geq 0.3$ \textbf{Include reference here}. Nevertheless, for $d < 0.3$
the implementation becomes slow given that more Bernoulli trials are
required in total for all random variables to generate a coded packet.
Thus, we introduce a second method that permits a fast implementation
for low coding densities \textbf{Same reference here}.

For this method, we first obtain the amount of packets
that we will combine to create a coded packet. To do it so, a random
number $\mathbb{N}$ of the original packets is used to produce a
coded packet. For our case, $\mathbb{N}$ is binomially distributed
with parameters $g$ for the number of trials and $d$ for its success
probability, e.g. $\mathbb{N} \sim Binomial(g,d)$. However,
when sampling from this distribution, the case
$\mathbb{N} = 0$ occurs with a non-zero probability. In order to handle
this special case, our implementation considers
$\mathbb{K} = \max(1,\mathbb{N})$ as the final amount of packets to be
mixed together. In this way, we always ensure that at least one packet is
encoded.

The only caveat is that the case of $\mathbb{K} = 1$ occurs slightly
more often than $\mathbb{N} = 1$ in the original distribution, but for
the considered density range in this method, this is not a significant
modification \textbf{Same reference here}. Then, once the distribution for the
number of packets to mix has been defined, we sample a value $n$ from
$\mathbb{N}$ and compute $k = \max(1,n)$. Later, we create a create a
set $\mathcal{K}$ with cardinality $k$, e.g. $|\mathcal{K}| = k$,
where the elements in $\mathcal{K}$ are the indexes of the packets that are
going to be considered for making a coded packet. To compute the indexes
of the set $\mathcal{K}$, we do the following algorithm in pseudo-code: \\
\ \\
\begin{algorithm}[H]
 \label{alg:k_set}
 \KwData{$k$: Size of $\mathcal{K}$. $g$: Generation Size}
 \KwResult{$\mathcal{K}$: The set of non-repeated indexes}
 $\mathcal{K}$ = \{ \}\;
 \While{$|\mathcal{K}| \neq k$ }{
  $i$ = Sample from $\mathbb{U}(1,g)$\;
  \If{$i \notin \mathcal{K}$}{
   Insert $i$ in $\mathcal{K}$\;
   }
 }
 \caption{Computation of the set of indexes for packet combination in SRLNC.}
\end{algorithm}
\ \\
Where in Algorithm~\ref{alg:k_set}, the notation $\mathbb{U}(1,g)$ stands
for a uniform discrete random variable with limits $1$ and $g$. The
pseudo-code in Algorithm~\ref{alg:k_set} indicates that the set $\mathbb{K}$
is filled with non-repeated elements taken uniformly at random from
$[1,\ldots,g]$. Finally, once the set of indexes has been defined, a coded
packet using \ac{SRLNC} can be written as shown in
\eqref{eq:sparse_coded_packet2}:
%
\begin{align} \label{eq:sparse_coded_packet2}
    C_i  &= \bigoplus_{m \in \mathcal{K}} v_{im} \otimes P_{m},\ v_{im} \neq 0,\ \forall i \in [1,\ldots)
    %L \subseteq \{1,\dots,g\}: |L|=k ,\
\end{align}

% It is often desired to express the coding density as $d=\frac{k}{g}$. We call
% this notation $k-sparse$ where $k$ is the average number of packets combined to
% produce a coded packet.

%For low coding density, the encoder might generate a coded packet that consists
%of no coded packets. I.e. the packet consists of all zeroes

%In \ac{SRLNC}, instead of considering all the packets to create a coded
%packet like \ac{RLNC} does, here the encoder sets some coding coefficients to
%zero when creating a new coded packet. This permits to reduce the complexity
%of both the encoding and decoding operations since the encoder (or decoder)
%requires less operations to process the data. Therefore, a new code parameter
%to control the sparsity, the code density $d$, is included to vary the
%number of non-zero packet that are combined. Here are various approaches to
%set the density.
%
%A first method is to specify the number of zeros in a given
%generation size and later generate coded packets with this amount. For example:
%16 zeros in a coding vector of 64 packets where the position of zeros are
%modified at random for every coded packet generated. Another method is to
%define an average number of zeros in the coding vector. Therefore, more or
%less zeros are exactly generated for each coded packet in the process.
%However, if not carefully made, using a sparse coding scheme might increase
%the probability of generating \ac{l.d.} packets which incurs both overhead
%from redundant information and processing delay.

\subsection{Tunable Sparse Network Coding}

\ac{TSNC} can be considered an extension to \ac{SRLNC}. The overall idea is
not only for the encoder to generate sparse coded packets, but also to
choose the sparsity of packets wiser. That is, as the decoder accomulates more
coded packets it becomes more and more likely that the next received
coded packet will be \ac{l.d.}.

The idea of \ac{TSNC} is therefore to gradually increase the coding density
as the decoderes \ac{DOF} increases. This enables TSNC to save a lot of
complexity particulary in the beginning of transmissions, but also to
tailor the coding delay. I.e. approximate the number of coded packets a
decoder needs to receive to decode a generation of $g$ packets. We call
this number the budget, $B$.

Provided a desired budget, $B \geq g$, and the decoders \ac{DOF} it can be
estimated how sparse the coded packets should be to finish a transmission in
time.

In our implementation, it is the encoders responsibility to estimate the coding
density. We use feedback to provide the encoder a better estimate of the
decoders \ac{DOF} at pre-defined points. Namely, when the decoder obtains the
following \ac{DOF}'s:

\begin{equation}\label{eq:feedback_dofs}
    s(k) = g\cdot \frac{2^k -1}{2^k},  \; \text{for } k=0,1,2,\dots
\end{equation}
where $k$ is the $k$-th feedback packet that is transmitted during a generation.

The encoder considers the time between feedback packets as regions. It then
estimates a fixed coding density for each region when a feedback packet is
received.

Before the coding density can be estimated it is essential that the encoder
has a good estimate of the decoders \ac{DOF}, but also the remainder of the
total budget. The feedback scheme \ref{eq:feedback_dofs} roughly splits the
remaining \ac{DOF} in halves. This is also the case for the budget that is
split equally among the two new regions. The very last region will be assigned
the full remainder of the total budget.

\subsection{Network Coding Implementation for Multicore Architectures}
\label{sub:implementation-multicore}

The operations needed to encode and decode data are, in general, similar. To
encode packets, the encoder needs to perform the matrix multiplication
$\textbf{C} = \textbf{V} \cdot \textbf{P}$. On the other hand, decoding the
information requires the decoder to find $\textbf{V}^{-1}$ and to perform the
matrix multiplication $\textbf{P} = \textbf{V}^{-1} \cdot \textbf{C}$. In both
cases, a matrix multiplication is needed. Therefore, to make a practical
implementation of network coding, it is valuable to find a way to optimize the
matrix multiplications operations for multicore architectures.

When designing multithreaded algorithms for network coding operations, it is
possible to implement the decoding by combining the matrix inversion and the
matrix multiplication, e.g., performing the Gauss-Jordan algorithm over the
coding matrix $\textbf{V}$ while performing, in parallel, row operations on the
coded data $\textbf{C}$. For example, in~\cite{5061951} and~\cite{4262451} the
parallelization of the row operations are optimized for \ac{GPU} and \ac{SMP}
systems respectively. However, the parallelization of such operations provide
limited speeds up for small block sizes ($\leq 2048$ bytes). The reason is that
to operate in a parallel fashion over the same coded packet $C_i$ requires
strained synchronization.

Therefore, to overcome the constraints of a tight synchronization, it is
preferable to explicitly invert the matrix $\textbf{V}$, and then take advantage
of optimizations for matrix multiplications, both at encoding and decoding time.
With that purpose, the authors implemented in~\cite{wunderlich2015network} an
algorithm that adopts the ideas of efficient \ac{BLAS}
operations~\cite{lawson1979basic} reimplementing them for finite fields
operations. Although there are already certain libraries available such as
~\cite{dumas2008dense} and \cite{dumas2002linbox} that allows the usage of the
highly optimize \ac{BLAS} implementations for some finite fields, they work
converting the GF elements into floating point numbers and back. Even though
that approach is efficient for large matrix sizes, the overhead imposed by the
conversions is not suitable for the the matrix sizes of typical network coding
implementations. The implemented algorithm aims to be cache efficient by
maximizing the number of operations performed over a block of fetched data. This
is done by dividing the matrices in square sub-blocks and operating with them.
As a consequence, this technique exploits the spatial locality of the data, at
least for $\mathcal{O}(n^3)$ algorithms~\cite{golub2012matrix}. The optimal size
of the block is architecture-dependent. The ideal block has a number of operands
that fit into L1 cache, and it is a multiple of the \ac{SIMD} operations size.

The main idea of the implemented algorithm is to represent each one of the sub-
block matrix operations (matrix-matrix multiplication, matrix-triangle matrix
multiplication, triangle-matrix system solving, etc) as a base, \textit{kernel},
operation that can be optimized individually, e.g., using \ac{SIMD} operations.
Each of these kernel operations, at the same time, can be represented as a task
with inputs and outputs in memory. If any algorithm is formulated
conventionally, and the data operations on the sub-blocks are reordered, then it
is possible to resolve data dependencies with a \ac{DAG}. For the algorithm, a
dependency means, for example, that a sub-block of the matrix in memory cannot
be read unless a previous writing operation was performed there. When the
\ac{DAG} is determined, then, a scheduler assigns to the available cores the
execution of the individual kernel operations that have their dependencies
satisfied. The benefit of this method is that the synchronization relies only on
data dependencies, and it does not requires the insertion of artificial
synchronization points. Using this technique, the matrix inversion is performed
using an algorithm based on LU
factorization~\cite{Dongarra:2011:HPM:2132876.2132885}, and the matrix
multiplication is performed by making the various matrix-matrix multiplications
on the sub-blocks.
